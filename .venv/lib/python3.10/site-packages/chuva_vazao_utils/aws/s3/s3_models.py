from __future__ import annotations

import os
from typing import List, Union
from datetime import datetime
from tempfile import mkdtemp
from dataclasses import field, dataclass

import boto3
from boto3.s3.transfer import TransferConfig
from botocore.exceptions import ClientError

from chuva_vazao_utils.utils.logger import logger
from chuva_vazao_utils.entities.apis.file import File, Folder


@dataclass
class S3Folder(Folder):
    _bucket_name: str
    _client: boto3.client = field(init=False)
    _s3_resource: boto3.resource = field(init=False)
    _bucket: boto3.resource = field(init=False)

    def __post_init__(self) -> None:
        self._client = boto3.client('s3')
        self._s3_resource = boto3.resource('s3')
        self._bucket = self._s3_resource.Bucket(self._bucket_name)

    @property
    def bucket_name(self) -> str:
        return self._bucket_name

    @bucket_name.setter
    def bucket_name(self, bucket_name: str):
        self._bucket_name = bucket_name
        self._bucket = self._s3_resource.Bucket(self._bucket_name)

    @property
    def uri(self):
        return f's3://{self._bucket_name}/{self.base_path}'

    def __eq__(self, another_folder: S3Folder) -> bool:
        return self.uri == another_folder.uri

    def copy(self) -> S3Folder:
        return S3Folder(self.base_path, self._bucket_name)

    def exists(self) -> bool:
        folder_path = self.base_path.rstrip('/')
        resp = self._client.list_objects(Bucket=self._bucket_name, Prefix=folder_path, Delimiter='/', MaxKeys=1)
        return 'CommonPrefixes' in resp

    def download(self, local_dst_folder: str):
        config = TransferConfig(max_concurrency=5)
        path = self.base_path if self.base_path.endswith('/') else f'{self.base_path}/'

        s3_objs = self._bucket.objects.filter(Prefix=path)

        for obj in s3_objs:
            filename = obj.key.split('/')[-1]
            filepath = os.path.join(local_dst_folder, filename)
            self._client.download_file(self._bucket_name, obj.key, filepath, Config=config)

    def upload(self, local_src_folder: str):
        self._sync_local_s3(local_src_folder, s3_source=False)

    def _sync_local_s3(self, local_folder_path: str, s3_source=False):
        src = self.uri if s3_source else local_folder_path
        dst = local_folder_path if s3_source else self.uri

        os.system(f'aws s3 sync {src} {dst} --acl bucket-owner-full-control')

    def delete(self):
        self._bucket.objects.filter(Prefix=self.base_path).delete()

    def list_contents(self, include_files: bool = True, include_folders: bool = True) -> List[Union[S3Folder, S3File]]:
        contents = []
        files = []
        for content in self._bucket.objects.filter(Prefix=self.base_path).all():
            logger.info(content.key)
            if content.key.endswith('/'):
                if include_folders:
                    contents.append(
                        S3Folder(content.key, self._bucket_name)
                    )
            else:
                if include_files:
                    contents.append(
                        S3File(content.key.rsplit('/', 1)[0], content.key.rsplit('/', 1)[-1], self._bucket_name)
                    )
            files.append(content)
        return contents

    def list_keys(self) -> List[str]:
        return [obj.key for obj in self._bucket.objects.filter(Prefix=self.base_path).all()]

    def get_oldest_file_datetime(self) -> datetime:
        return self._find_last_file_datetime(False)

    def get_newest_file_datetime(self) -> datetime:
        return self._find_last_file_datetime(True)

    def _find_last_file_datetime(self, newest: bool = False) -> Union[None, datetime]:
        def get_last_modified(obj):
            return int(obj.last_modified.strftime('%s'))

        s3_objects = [f for f in self._bucket.objects.filter(Prefix=self.base_path).all()]
        if not s3_objects:
            logger.warning(f'Nenhum arquivo encontrado no diretório "{self.uri}"')
            return
        last_added = sorted(s3_objects, key=get_last_modified, reverse=newest)[0]
        return last_added.last_modified

    def copy_contents_from(self, src_folder: S3Folder):
        if self == src_folder:
            return
        os.system(f"aws s3 sync {src_folder.uri} {self.uri} --acl bucket-owner-full-control")


@dataclass
class S3File(File):
    bucket_name: str
    _client: boto3.client = field(init=False)
    _s3_resource: boto3.resource = field(init=False)

    def __post_init__(self) -> None:
        self._client = boto3.client('s3')
        self._s3_resource = boto3.resource('s3')

    @classmethod
    def create_from_key_bucket(cls, key: str, bucket_name: str) -> S3File:
        split_key = key.rsplit('/', 1)
        folder = split_key[0]
        filename = split_key[1]
        return S3File(base_path=folder, filename=filename, bucket_name=bucket_name)

    @property
    def uri(self):
        return f's3://{self.bucket_name}/{self.filepath}'

    def __eq__(self, another_folder: S3Folder) -> bool:
        return self.uri == another_folder.uri

    @property
    def key(self) -> str:
        return f'{self.base_path}/{self.filename}'

    @property
    def folder(self) -> S3Folder:
        return S3Folder(self.base_path, self.bucket_name)

    def exists(self) -> bool:
        s3_objects = self._client.list_objects_v2(Bucket=self.bucket_name, Prefix=self.filepath)
        return bool(s3_objects.get("Contents"))

    def download(self, local_folder: str = None) -> Union[str, None]:
        """
        Faz o download do arquivo do s3 e retorna o caminho do arquivo.

        Args:
            local_folder (str, optional): pasta para a qual o arquivo será baixado. Defaults to None (uma pasta será criada).

        Raises:
            e: Exceções quando há erro desconhecido no download(Ex.: Falha na conexão, sem permissão, ...)

        Returns:
            Union[str, None]: str: Caminho do arquivo baixado
                              None: Se o arquivo não foi encontrado no s3
        """
        if not local_folder:
            local_folder = mkdtemp()

        local_filepath = os.path.join(local_folder, self.filename)

        try:
            self._client.download_file(self.bucket_name, self.filepath, local_filepath)
            return local_filepath
        except ClientError as e:
            if 'Not Found' in str(e):
                return None
            raise e

    def read_content(self) -> Union[List[str], None]:
        """Faz o download do arquivo do s3 e retorna o conteúdo do arquivo.

        Args:
            local_folder (str): pasta para a qual o arquivo será baixado

        Raises:
            e: Exceções quando há erro desconhecido no download (Ex.: Falha na conexão, sem permissão, ...)

        Returns:
             Union[List[str], None]: List[str]: Lista com cada linha do arquivo
                                     None: Se o arquivo não foi encontrado no s3
        """
        local_filepath = self.download()
        if local_filepath:
            with open(local_filepath, 'r') as file:
                file_lines = file.read().splitlines()
                return [line.strip() for line in file_lines]

    def upload(self, local_filepath: str):
        self._s3_resource.Bucket(self.bucket_name).upload_file(local_filepath, self.filepath)

    def save_content(self, file_content: str):
        tmp_dir = mkdtemp()
        local_filepath = os.path.join(tmp_dir, 'tmpfile.txt')
        with open(local_filepath, 'w', encoding='utf-8') as f:
            f.write(file_content)

        self.upload(local_filepath)

    def delete(self):
        self._s3_resource.Object(self.bucket_name, self.filepath).delete()

    def copy_from(self, src_file: S3File):
        if self == src_file:
            return
        os.system(f'aws s3 cp {src_file.uri} {self.uri} --acl bucket-owner-full-control')
